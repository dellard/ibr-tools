{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2020-2024 - Raytheon BBN Technologies Corp.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "\n",
    "You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0.\n",
    "\n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an\n",
    "\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n",
    "either express or implied. See the License for the specific\n",
    "language governing permissions and limitations under the License.\n",
    "\n",
    "Distribution Statement \"A\" (Approved for Public Release,\n",
    "Distribution Unlimited).\n",
    "\n",
    "This material is based upon work supported by the Defense\n",
    "Advanced Research Projects Agency (DARPA) under Contract No.\n",
    "HR001119C0102.  The opinions, findings, and conclusions stated\n",
    "herein are those of the authors and do not necessarily reflect\n",
    "those of DARPA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtractUniqueSourceAddresses\n",
    "This file reads all the unique source addresses in the given meanie data and prints two text files that contain:\n",
    "1. IPs we don't have geolocation for\n",
    "2. IPs we don't have device info for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipaddress\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set value to 1 to see intermediate outputs for debugging. 0 otherwise (recommended)\n",
    "debug = 0\n",
    "\n",
    "# Specify the /24 address of interest as a.b.c\n",
    "destination_address_of_interest = '10.1.202'\n",
    "\n",
    "# (Optional) Set 1 if you have an existing Geolocations database to merge, 0 otherwise\n",
    "existing_geolocations_db = 0\n",
    "\n",
    "# (Optional) Location of the current geolocation database file with path, must be specified if previous input is 1\n",
    "geo_location_db_file = 'db/full_geolocations_db.csv'\n",
    "\n",
    "# (Optional) Set 1 if you have an existing device types database to merge, 0 otherwise\n",
    "existing_device_types_db = 0\n",
    "\n",
    "# (Optional) Location of the current device types database file with path, must be specified if previous input is 1\n",
    "device_types_db_file = 'db/full_devices_db.csv'\n",
    "\n",
    "# Directory where results are printed\n",
    "output_dir_path = r'outputs'\n",
    "\n",
    "# File name of the file that will contain addresses missing geolocation \n",
    "ipsForGeoLocation = 'IPsForGeoLocation.txt'\n",
    "\n",
    "# File name of the file that will contain addresses missing device types  \n",
    "ipsForDeviceTypes = 'IPsForDeviceTypes.txt'\n",
    "\n",
    "# years in the data that should be analyzed\n",
    "years_of_interest = [2020, 2021, 2022, 2023]\n",
    "\n",
    "# Directory where meanie text data is stored\n",
    "input_dir_path = r'/home/nice-user/Yearly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, Format, Filter, and Iterate over all the Meanie Data of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all 256 /24 addresses for the given subnet\n",
    "all_dest_addresses = [f'{destination_address_of_interest}.{x}' for x in range(0,256)]\n",
    "\n",
    "if(input_dir_path[len(input_dir_path)-1] != '/'):\n",
    "    input_dir_path = input_dir_path + '/'\n",
    "\n",
    "if(output_dir_path[len(output_dir_path)-1] != '/'):\n",
    "    output_dir_path = output_dir_path + '/'\n",
    "    \n",
    "if debug == 1:\n",
    "    print(input_dir_path)\n",
    "    print(output_dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "orig_data = pd.DataFrame()\n",
    "data_stats = {}\n",
    "stat_columns = ['Source_Address', 'Destination_Address', 'TTL', 'Destination_Port', 'Payload_Length']\n",
    "hex_columns = ['Source_Address', 'Destination_Address', 'Source_Port', 'Destination_Port', 'UDP_Checksum', 'TTL', 'IPID']\n",
    "ip_columns = ['Source_Address', 'Destination_Address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute and store statistics of data before and after filtering\n",
    "def compute_data_stats(orig_data, years_of_interest, stat_columns, prefix):\n",
    "    for year in years_of_interest:\n",
    "        current_year_data = orig_data[orig_data['Year'] == year]\n",
    "        data_stats[year] = {f'{prefix}_Packet_Counts': len(current_year_data)}\n",
    "        for column in stat_columns:\n",
    "            data_stats[year][f'{prefix}_Unique_{column}_Counts'] = current_year_data[column].nunique()\n",
    "            data_stats[year][f'{prefix}_Unique_{column}s'] = current_year_data[column].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# values in Input data\n",
    "\n",
    "# Saddr - Hex\n",
    "# Daddr - Hex\n",
    "# Sport - Hex\n",
    "# Dport - Hex\n",
    "# Proto – always UDP (17)\n",
    "# Timestamp\n",
    "# UdpCksum - Hex\n",
    "# PayloadLen – the length of the payload, in bytes (the UDP length field, minus 8 for the UDP header itself)\n",
    "# Payload – Hex\n",
    "# TTL - a hex\n",
    "# IPID - a hex\n",
    "\n",
    "print(\"Reading Meanie Data\")\n",
    "\n",
    "# list to store files\n",
    "file_list = []\n",
    "\n",
    "try:\n",
    "    all_files = os.listdir(input_dir_path)\n",
    "except:\n",
    "    sys.exit('Problem with input directory. Check if the path is correct')\n",
    "\n",
    "# Iterate directory and find all compatible files\n",
    "for file in all_files:\n",
    "    # check only text files and add them to the list\n",
    "    if file.endswith('.txt'):\n",
    "        file_list.append(file)\n",
    "\n",
    "# if there is 1 or more file type of interest\n",
    "if len(file_list)>=1:\n",
    "\n",
    "    for file in range(0, len(file_list)):\n",
    "        full_file_path = input_dir_path+file_list[file]\n",
    "        \n",
    "        if debug == 1:\n",
    "            print('full_file_path: ', full_file_path)\n",
    "\n",
    "\n",
    "        # if filesize > 0, i.e. file is not empty\n",
    "        if os.path.getsize(full_file_path) > 0:\n",
    "            current_csv = pd.read_csv(full_file_path, header=None)\n",
    "            current_csv['FileName'] = file_list[file]\n",
    "            orig_data = pd.concat([orig_data, current_csv], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    sys.exit(\"No compatible Files (.txt) in the directory\")\n",
    "\n",
    "if debug == 1:\n",
    "    display(orig_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if the data is not empty\n",
    "if len(orig_data)>=1:\n",
    "\n",
    "    # Format all the read data\n",
    "    orig_data = orig_data.rename(columns={0: 'Source_Address', 1:'Destination_Address', 2:'Source_Port', \n",
    "                                          3:'Destination_Port', 4:'Protocol', 5:'Timestamp', \n",
    "                                          6:'UDP_Checksum', 7:'Payload_Length', 8:'Payload', 9:'TTL', 10:'IPID'})\n",
    "    \n",
    "    for hc in hex_columns:\n",
    "        orig_data[hc] = orig_data[hc].apply(lambda x: int(x, 16))\n",
    "        \n",
    "    for ic in ip_columns:\n",
    "        orig_data[ic] = orig_data[ic].apply(lambda x: str(ipaddress.ip_address(x)))\n",
    "        \n",
    "    orig_data[['Prefix', 'Year', 'Month', 'Day', 'Hour', 'Post']] = orig_data['FileName'].str.split('-', expand=True)\n",
    "    orig_data = orig_data.drop(['Prefix', 'Post'], axis=1)\n",
    "    orig_data[['Year', 'Month', 'Day', 'Hour']] = orig_data[['Year', 'Month', 'Day', 'Hour']].astype(int)\n",
    "    \n",
    "    if debug == 1:\n",
    "        display(orig_data)\n",
    "    \n",
    "    # Extract Stats about the full data\n",
    "    compute_data_stats(orig_data, years_of_interest, stat_columns, 'All')\n",
    "    \n",
    "    if debug == 1:            \n",
    "        print('Before Filtering')\n",
    "        for year in years_of_interest:\n",
    "            print(f'{year} Num Unique Destination Addresses: ', data_stats[year]['All_Unique_Destination_Address_Counts'])\n",
    "            print(f'{year} Unique Destination Addresses: ', data_stats[year]['All_Unique_Destination_Addresss'])\n",
    "    \n",
    "    \n",
    "    # filter the data by address of interest\n",
    "    orig_data = orig_data[orig_data[\"Destination_Address\"].isin(all_dest_addresses)]\n",
    "\n",
    "    if debug == 1:\n",
    "        display(orig_data)\n",
    "    \n",
    "    # Extract Stats about the filtered data\n",
    "    compute_data_stats(orig_data, years_of_interest, stat_columns, 'Filtered')\n",
    "    \n",
    "    if debug == 1:            \n",
    "        print('\\nAfter Filtering')\n",
    "        for year in years_of_interest:\n",
    "            print(f'{year} Num Unique Destination Addresses: ', data_stats[year]['Filtered_Unique_Destination_Address_Counts'])\n",
    "            print(f'{year} Unique Destination Addresses: ', data_stats[year]['Filtered_Unique_Destination_Addresss'])\n",
    "\n",
    "else:\n",
    "    sys.exit('\\nInput Meanie Data is Empty.\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out all the Source Addresses without GeoLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if existing_geolocations_db == 1:\n",
    "    print(\"\\nComparing to Existing GeoLocations Database\\n\")\n",
    "    \n",
    "    try:\n",
    "        source_geo_locations = pd.read_csv(geo_location_db_file)\n",
    "        source_geo_locations = source_geo_locations.rename(columns={'IP': 'Source_Address'})\n",
    "\n",
    "        if debug == 1:\n",
    "            source_geo_locations.head()\n",
    "            source_geo_locations.info()\n",
    "    \n",
    "    except:\n",
    "        sys.exit(f'\\nProblem reading geolocations database file ({geo_location_db_file}). Check if the path is correct\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrinting Geolocations File\\n\")\n",
    "\n",
    "if existing_geolocations_db == 1:\n",
    "    try:\n",
    "        remaining_data = orig_data.loc[~orig_data['Source_Address'].isin(source_geo_locations['Source_Address']), 'Source_Address']\n",
    "        remaining_data.to_csv(f'{output_dir_path}{ipsForGeoLocation}', header=False, index=False)\n",
    "    except:\n",
    "        print('\\nProblem writing geolocations IP file. Check if the output path is correct\\n')\n",
    "else: \n",
    "    try:\n",
    "        remaining_data = orig_data['Source_Address']\n",
    "        remaining_data.to_csv(f'{output_dir_path}{ipsForGeoLocation}', header=False, index=False)\n",
    "    except:\n",
    "        print('\\nProblem writing geolocations IP file. Check if the output path is correct\\n')\n",
    "\n",
    "if debug == 1:\n",
    "    print('len(remaining_data): ', len(remaining_data))\n",
    "    display(remaining_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out all the Source Addresses without Device Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if existing_device_types_db == 1:\n",
    "    print(\"\\nComparing to Existing Device Types Database\\n\")\n",
    "    \n",
    "    try:\n",
    "        source_devices = pd.read_csv(device_types_db_file)\n",
    "\n",
    "        if debug == 1:\n",
    "            source_devices.head()\n",
    "            source_devices.info()\n",
    "    except:\n",
    "        sys.exit(f'\\nProblem reading device types database file ({device_types_db_file}). Check if path is correct\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrinting Device Types File\\n\")\n",
    "if existing_device_types_db == 1:    \n",
    "    try:\n",
    "        remaining_data = orig_data.loc[~orig_data['Source_Address'].isin(source_devices['IP']), 'Source_Address']\n",
    "        remaining_data.to_csv(f'{output_dir_path}{ipsForDeviceTypes}', header=False, index=False)\n",
    "    except:\n",
    "        print('\\nProblem writing device types IP file. Check if the output path is correct\\n')\n",
    "else:\n",
    "    try:\n",
    "        remaining_data = orig_data['Source_Address']\n",
    "        remaining_data.to_csv(f'{output_dir_path}{ipsForDeviceTypes}', header=False, index=False)\n",
    "    except:\n",
    "        print('\\nProblem writing device types IP file. Check if the output path is correct\\n')\n",
    "\n",
    "if debug == 1:\n",
    "    print('len(remaining_data): ', len(remaining_data))\n",
    "    display(remaining_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
