{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2020-2024 - Raytheon BBN Technologies Corp.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "\n",
    "You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0.\n",
    "\n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an\n",
    "\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n",
    "either express or implied. See the License for the specific\n",
    "language governing permissions and limitations under the License.\n",
    "\n",
    "Distribution Statement \"A\" (Approved for Public Release,\n",
    "Distribution Unlimited).\n",
    "\n",
    "This material is based upon work supported by the Defense\n",
    "Advanced Research Projects Agency (DARPA) under Contract No.\n",
    "HR001119C0102.  The opinions, findings, and conclusions stated\n",
    "herein are those of the authors and do not necessarily reflect\n",
    "those of DARPA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greenwich Meanie Yearly Analysis on a Specified /24 Destination Subnet\n",
    "\n",
    "Runs analysis on the Meanine Geolocation data for a given subnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import ipaddress\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignoring warnings, otherwise there are warnings about having too many graphs\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set value to 1 to see intermediate outputs for debugging. 0 otherwise (recommended)\n",
    "debug = 0\n",
    "\n",
    "# /24 of the destination addresses to be kept\n",
    "destination_address_of_interest = '10.1.202'\n",
    "\n",
    "# Set value to 1 to do geolocation analysis using results generated with 'ipinfo.io'. 0 otherwise (recommended)\n",
    "# If this is 1, add the path to the geolocations database in the next variable \n",
    "analyze_geolocation = 0\n",
    "\n",
    "# full path and filename that contains prior Geolocation database\n",
    "geo_location_db_file = './db/full_geolocations_db.csv'\n",
    "\n",
    "# Set value to 1 to analyze device types from lift. 0 otherwise (recommended)\n",
    "# If this is 1, add the path to the device types database in the next variable \n",
    "analyze_device_types = 0\n",
    "\n",
    "# full path and filename that contains prior device types database\n",
    "devices_db_file = './db/full_devices_db.csv'\n",
    "\n",
    "# Directory where meanie text data is stored\n",
    "input_dir_path = r'/home/nice-user/Yearly/'\n",
    "\n",
    "# Directory where meanie results are printed\n",
    "output_dir_path = r'/home/nice-user/Yearly'\n",
    "\n",
    "# Set value to 1 to save figures to output_dir_path. 0 otherwise\n",
    "save_figs = 1\n",
    "\n",
    "# Years in the data to be analyzed\n",
    "years_of_interest = [2020, 2021, 2022, 2023]\n",
    "\n",
    "# Countries user wants to extract Information for\n",
    "countries_of_interest = [\"United Kingdom\", \"Philippines\", \"Brazil\", \"India\", \"Greece\", \"United States\", \"Canada\", \n",
    "                         \"Serbia\", \"Poland\", \"India\", \"Croatia\", \"Indonesia\", \"South Africa\", \"China\", \"Russia\", \n",
    "                         \"Singapore\", \"Argentina\", \"Australia\", \"Czech Republic\", \"Bangladesh\", \"France\", \"Malaysia\", \"Turkey\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Variables and Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns in the meanie text files that have data as hex\n",
    "hex_columns = ['Source_Address', 'Destination_Address', 'Source_Port', 'Destination_Port', 'UDP_Checksum', 'TTL', 'IPID']\n",
    "\n",
    "# Columns from meanie text that will need to be formatted as IP addresses\n",
    "ip_columns = ['Source_Address', 'Destination_Address']\n",
    "\n",
    "# Columns for computing before and after filtering statistics\n",
    "stat_columns = ['Source_Address', 'Destination_Address', 'TTL', 'Destination_Port', 'Payload_Length', 'Destination_Slash24']\n",
    "\n",
    "orig_data = pd.DataFrame()\n",
    "data_stats = {}\n",
    "\n",
    "for year in years_of_interest:\n",
    "    data_stats[year] = {}\n",
    "\n",
    "if(input_dir_path[len(input_dir_path)-1] != '/'):\n",
    "    input_dir_path = input_dir_path + '/'\n",
    "    \n",
    "if(output_dir_path[len(output_dir_path)-1] != '/'):\n",
    "    output_dir_path = output_dir_path + '/'\n",
    "\n",
    "all_dest_addresses = [f'{destination_address_of_interest}.{x}' for x in range(0,256)]\n",
    "\n",
    "if debug == 1:\n",
    "    print(input_dir_path)\n",
    "    print(data_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes Stats on the data before and after filtering\n",
    "\n",
    "def compute_data_stats(orig_data, years_of_interest, stat_columns, prefix):\n",
    "    for year in years_of_interest:\n",
    "        current_year_data = orig_data[orig_data['Year'] == year]\n",
    "        current_year_unique_months = current_year_data['Month'].nunique()\n",
    "        if debug == 1:\n",
    "            print(year, ' unique months: ', current_year_unique_months)\n",
    "        data_stats[year][f'{prefix}_Packet_Counts'] = len(current_year_data)\n",
    "        data_stats[year][f'{prefix}_Packet_norm'] = data_stats[year][f'{prefix}_Packet_Counts'] / current_year_unique_months\n",
    "        for column in stat_columns:\n",
    "            data_stats[year][f'{prefix}_Unique_{column}_Counts'] = current_year_data[column].nunique()\n",
    "            data_stats[year][f'{prefix}_Unique_{column}s'] = current_year_data[column].unique()\n",
    "            data_stats[year][f'{prefix}_Unique_{column}_norm'] = data_stats[year][f'{prefix}_Unique_{column}_Counts'] / current_year_unique_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read / Clean / Format / Merge Data and Compute Filtering Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# values in Input data\n",
    "\n",
    "# Saddr - Hex\n",
    "# Daddr - Hex\n",
    "# Sport - Hex\n",
    "# Dport - Hex\n",
    "# Proto – always UDP (17)\n",
    "# Timestamp\n",
    "# UdpCksum - Hex\n",
    "# PayloadLen – the length of the payload, in bytes (the UDP length field, minus 8 for the UDP header itself)\n",
    "# Payload – Hex\n",
    "# TTL - a hex\n",
    "# IPID - a hex\n",
    "\n",
    "print(\"Reading Meanie Data\")\n",
    "\n",
    "# list to store files\n",
    "file_list = []\n",
    "\n",
    "# Iterate directory and find all compatible files\n",
    "for file in os.listdir(input_dir_path):\n",
    "    # check only text files and add them to the list\n",
    "    if file.endswith('.txt'):\n",
    "        file_list.append(file)\n",
    "\n",
    "# if there is 1 or more file type of interest\n",
    "if len(file_list)>=1:\n",
    "\n",
    "    for file in range(0, len(file_list)):\n",
    "        full_file_path = input_dir_path+file_list[file]\n",
    "\n",
    "        # Read data from the file only if the file is not empty, i.e. file size > 0\n",
    "        if os.path.getsize(full_file_path) > 0:\n",
    "            current_csv = pd.read_csv(full_file_path, header=None)\n",
    "            current_csv['FileName'] = file_list[file]\n",
    "            orig_data = pd.concat([orig_data, current_csv], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    print(\"No compatible Files in the directory\")\n",
    "\n",
    "if debug == 1:\n",
    "    display(orig_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data is not empty\n",
    "if len(orig_data)>=1:\n",
    "\n",
    "    # Format all the read data\n",
    "    orig_data = orig_data.rename(columns={0: 'Source_Address', 1:'Destination_Address', 2:'Source_Port', \n",
    "                                          3:'Destination_Port', 4:'Protocol', 5:'Timestamp', \n",
    "                                          6:'UDP_Checksum', 7:'Payload_Length', 8:'Payload', 9:'TTL', 10:'IPID'})\n",
    "    \n",
    "    for hc in hex_columns:\n",
    "        orig_data[hc] = orig_data[hc].apply(lambda x: int(x, 16))\n",
    "        \n",
    "    for ic in ip_columns:\n",
    "        orig_data[ic] = orig_data[ic].apply(lambda x: str(ipaddress.ip_address(x)))\n",
    "        \n",
    "    orig_data[['Prefix', 'Year', 'Month', 'Day', 'Hour', 'Post']] = orig_data['FileName'].str.split('-', expand=True)\n",
    "    orig_data = orig_data.drop(['Prefix', 'Post'], axis=1)\n",
    "    orig_data[['Year', 'Month', 'Day', 'Hour']] = orig_data[['Year', 'Month', 'Day', 'Hour']].astype(int)\n",
    "    orig_data['Destination_Slash24'] = orig_data['Destination_Address'].apply(lambda row: '.'.join(row.split('.')[:3]))\n",
    "\n",
    "    \n",
    "    if debug == 1:\n",
    "        display(orig_data)\n",
    "    \n",
    "    # Extract Stats about the full data\n",
    "    compute_data_stats(orig_data, years_of_interest, stat_columns, 'All')\n",
    "    \n",
    "    if debug == 1:\n",
    "        print('Before Filtering')\n",
    "        for year in years_of_interest:\n",
    "            print(f'{year} Num Unique Destination Addresses: ', data_stats[year]['All_Unique_Destination_Address_Counts'])\n",
    "    \n",
    "    # Keep a copy of the full data without filtering\n",
    "    unfiltered_data = orig_data\n",
    "    \n",
    "    \n",
    "    # filter the data by address of interest\n",
    "    orig_data = orig_data[orig_data[\"Destination_Address\"].isin(all_dest_addresses)]\n",
    "    # Extract Stats about the filtered data\n",
    "    compute_data_stats(orig_data, years_of_interest, stat_columns, 'Filtered')\n",
    "    \n",
    "    if debug == 1:\n",
    "        print('\\nAfter Filtering')\n",
    "        for year in years_of_interest:\n",
    "            print(f'{year} Num Unique Destination Addresses: ', data_stats[year]['Filtered_Unique_Destination_Address_Counts'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Unique source counts by year and country\n",
    "packets_by_year_slash24 = unfiltered_data.groupby(['Year', 'Destination_Slash24'])['Source_Address'].count().reset_index()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "pd.pivot_table(packets_by_year_slash24, values = 'Source_Address', index = ['Destination_Slash24'], columns = ['Year']).plot.bar(figsize=(12,6), title='Greenwich Meanie Packets for each /24 by Year')\n",
    "\n",
    "plt.xlabel(\"Destination /24\")\n",
    "plt.ylabel(\"Packet Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}MeaniePacketsByYearAndSlash24.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_stats_df = pd.DataFrame(data_stats).transpose()\n",
    "if debug == 1:\n",
    "    display(filter_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Sources and Packets by Years\n",
    "fig = plt.figure()\n",
    "filter_stats_df['All_Unique_Source_Address_Counts'].plot.bar( title='All Unique Source Address Counts by Year')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Unique Source Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_sourceAddresses_yearly.png')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['Filtered_Unique_Source_Address_Counts'].plot.bar(title=f'{destination_address_of_interest} Unique Source Address Counts by Year')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Unique Source Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_sourceAddresses_yearly_filtered.png')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['Filtered_Unique_Source_Address_norm'].plot.bar( title=f'{destination_address_of_interest} Yearly Unique Source Address Counts Normalized by Months')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Normalized Unique Source Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_sourceAddresses_yearly_filtered_normalized.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['All_Packet_Counts'].plot.bar( title='All Packet Counts by Year')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Packet Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_packets_yearly.png')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['Filtered_Packet_Counts'].plot.bar(title=f'{destination_address_of_interest} Packet Counts by Year')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Packet Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_packets_yearly_filtered.png')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['Filtered_Packet_norm'].plot.bar( title=f'{destination_address_of_interest} Yearly Unique Packets Counts Normalized by Months')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Normalized Packet Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_packets_yearly_filtered_normalized.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at TTLs by Years\n",
    "fig = plt.figure()\n",
    "filter_stats_df['All_Unique_TTL_Counts'].plot.bar( title='All TTL Counts by Year')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Unique Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_TTLs_yearly.png')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['Filtered_Unique_TTL_Counts'].plot.bar(title=f'{destination_address_of_interest} TTL Counts by Year')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Unique Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_TTLs_yearly_filtered.png')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['Filtered_Unique_TTL_norm'].plot.bar(title=f'{destination_address_of_interest} Yearly Unique TTL Counts Normalized by Months')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Unique Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_TTLs_yearly_filtered_normalized.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Payload Lengths by Years\n",
    "fig = plt.figure()\n",
    "filter_stats_df['All_Unique_Payload_Length_Counts'].plot.bar( title='All Payload Length Counts by Year')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Unique Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_Payload_Lengths_yearly.png')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['Filtered_Unique_Payload_Length_Counts'].plot.bar(title=f'{destination_address_of_interest} Payload Length Counts by Year')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Unique Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_Payload_Lengths_yearly_filtered.png')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "filter_stats_df['Filtered_Unique_Payload_Length_norm'].plot.bar(title=f'{destination_address_of_interest} Yearly Unique Payload Length Counts Normalized by Months')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Unique Counts\")\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Unique_Payload_Lengths_yearly_filtered_normalized.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years_of_interest:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    current_table = orig_data[orig_data['Year']==year]\n",
    "    ax = current_table['TTL'].plot(kind='hist', bins = 100, title=f'{year} Distribution of TTLs in {destination_address_of_interest}')\n",
    "    ax.set_xlabel(\"TTLs\")\n",
    "    if save_figs == 1:\n",
    "        plt.savefig(f'{output_dir_path}TTL_distribution_{year}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payload Length Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years_of_interest:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    current_table = orig_data[orig_data['Year']==year]\n",
    "    vc = current_table['Payload_Length'].value_counts(sort = False)\n",
    "    ax = vc.plot(kind='bar', title=f'{year} Distribution of Payload Lengths in {destination_address_of_interest}', align='edge', width=1.0)\n",
    "    ax.set_xlabel(\"Payload Lengths\")\n",
    "    if save_figs == 1:\n",
    "        plt.savefig(f'{output_dir_path}Payload_Length_distribution_{year}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yearly Geolocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    continent_coordinates = {'World': {'xlim':[-180, 180], 'ylim':[-90, 90]},\n",
    "                             'Europe': {'xlim':[-20, 50], 'ylim':[25, 70]},\n",
    "                             'Asia': {'xlim':[60, 145], 'ylim':[-10, 50]},\n",
    "                             'North America': {'xlim':[-130, -60], 'ylim':[15, 60]},\n",
    "                             'South America': {'xlim':[-85, -30], 'ylim':[-55, 15]},\n",
    "                             'Africa': {'xlim':[-35, 60], 'ylim':[-40,40]},\n",
    "                             'Australia': {'xlim':[110, 187], 'ylim':[-50, -5]}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    print(\"Merging with GeoLocations data\")\n",
    "\n",
    "    database_geo_locations = pd.read_csv(geo_location_db_file)\n",
    "    database_geo_locations = database_geo_locations.rename(columns={'IP': 'Source_Address'})\n",
    "\n",
    "    if debug == 1:\n",
    "        display(database_geo_locations)\n",
    "        database_geo_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two tables \n",
    "if analyze_geolocation == 1:\n",
    "    merged_table = pd.merge(orig_data, database_geo_locations, on=\"Source_Address\", how = 'left')\n",
    "\n",
    "    if debug == 1:\n",
    "        display(merged_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    if debug == 1:\n",
    "        merged_table.info()\n",
    "        merged_table['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    # Delete unnecessary columns\n",
    "    subset_table = merged_table.drop(['Timestamp_y', 'Timestamp_y', 'Payload'], axis=1)\n",
    "\n",
    "    if debug == 1:\n",
    "        print('subset_table.size', subset_table.size)\n",
    "        subset_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    # Remove duplicate rows so each source exists only once for each year and location \n",
    "    subset_table = subset_table.drop_duplicates()\n",
    "\n",
    "    if debug == 1:\n",
    "        print('subset_table.size', subset_table.size)\n",
    "        display(subset_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    unique_source_counts_by_year_location = subset_table.groupby(['Year', 'Country', 'Region', 'City', 'Latitude', 'Longitude'])['Source_Address'].nunique().reset_index()\n",
    "    unique_source_counts_by_year_location.to_csv(f'{output_dir_path}CityCountsByYear.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    unique_years = unique_source_counts_by_year_location['Year'].unique()\n",
    "    max_count = unique_source_counts_by_year_location['Source_Address'].max()\n",
    "\n",
    "    continents = list(continent_coordinates.keys())\n",
    "    for current_year in unique_years:\n",
    "        current_year_df = unique_source_counts_by_year_location.loc[unique_source_counts_by_year_location['Year']==current_year]\n",
    "\n",
    "        for continent in continents:\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "            # Plotting world map\n",
    "            worldmap = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "            worldmap.plot(color=\"lightgrey\", ax=ax)\n",
    "            ax.set_axis_off()\n",
    "\n",
    "            longitudes = current_year_df['Longitude']\n",
    "            latitudes = current_year_df['Latitude']\n",
    "            counts = current_year_df['Source_Address']\n",
    "            # Plotting longitudes and latitudes\n",
    "            plt.scatter(longitudes, latitudes, s=counts, c=counts, alpha=0.6, vmin=0, vmax=max_count, cmap='autumn')\n",
    "            plt.colorbar(label=\"Count of Unique Source Addresses by City\")\n",
    "            plt.xlim(continent_coordinates[continent]['xlim'])\n",
    "            plt.ylim(continent_coordinates[continent]['ylim'])\n",
    "            plt.title(f'{continent} {current_year}')\n",
    "            if save_figs == 1:\n",
    "                plt.savefig(f'{output_dir_path}{continent}_map_{current_year}.png')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is the same chart as above, but each city is normalized by % of global traffic. \n",
    "# The values are so small that these graphs are not super useful. \n",
    "# Left code in here in case this useful for another set of data, but the figures are not saved to output directory\n",
    "if analyze_geolocation == 1:\n",
    "\n",
    "    unique_years = unique_source_counts_by_year_location['Year'].unique()\n",
    "\n",
    "    continents = list(continent_coordinates.keys())\n",
    "    for current_year in unique_years:\n",
    "        current_year_df = unique_source_counts_by_year_location.loc[unique_source_counts_by_year_location['Year']==current_year]\n",
    "        total_yearly_count = current_year_df['Source_Address'].sum()\n",
    "        current_year_df['Source_Address'] = current_year_df['Source_Address'].div(total_yearly_count)\n",
    "        max_count = current_year_df['Source_Address'].max()\n",
    "\n",
    "        for continent in continents:\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "            # Plotting world map\n",
    "            worldmap = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "            worldmap.plot(color=\"lightgrey\", ax=ax)\n",
    "            ax.set_axis_off()\n",
    "\n",
    "            longitudes = current_year_df['Longitude']\n",
    "            latitudes = current_year_df['Latitude']\n",
    "            counts = current_year_df['Source_Address']\n",
    "            # Plotting longitudes and latitudes\n",
    "            plt.scatter(longitudes, latitudes, s=counts, c=counts, alpha=0.6, vmin=0, vmax=max_count, cmap='autumn')\n",
    "            plt.colorbar(label=\"Annual Normalized Unique Source Addresses by City\")\n",
    "            plt.xlim(continent_coordinates[continent]['xlim'])\n",
    "            plt.ylim(continent_coordinates[continent]['ylim'])\n",
    "            plt.title(f'{continent} {current_year}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    # Unique source counts by year and country\n",
    "    unique_sources_by_year_country = subset_table.groupby(['Year', 'Country'])['Source_Address'].nunique().reset_index()\n",
    "\n",
    "    print('Number of unique countries = ', len(unique_sources_by_year_country['Country'].unique()))\n",
    "    unique_sources_by_year_country.to_csv(f'{output_dir_path}YearlyCountrySources.csv', index = False)\n",
    "\n",
    "    unique_sources_by_year_country = unique_sources_by_year_country.loc[unique_sources_by_year_country['Country'].isin(countries_of_interest)]\n",
    "\n",
    "\n",
    "    pd.pivot_table(unique_sources_by_year_country, values = 'Source_Address', index = ['Country'], columns = ['Year']).plot.bar(figsize=(18,6), title='Yearly Unique Source Addresses by Country')\n",
    "\n",
    "    if save_figs == 1:\n",
    "        plt.savefig(f'{output_dir_path}Unique_Country_SourceAddresses.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    \n",
    "    # Unique source counts by year and country - scaled by that year\n",
    "    unique_sources_by_year_country = subset_table.groupby(['Year', 'Country'])['Source_Address'].nunique().reset_index()\n",
    "\n",
    "    scaled_sources_by_year_country = pd.DataFrame()\n",
    "\n",
    "    for year in years_of_interest:\n",
    "        current_year_df = unique_sources_by_year_country[unique_sources_by_year_country['Year']==year]\n",
    "        total_yearly_count = current_year_df['Source_Address'].sum()\n",
    "        current_year_df['Source_Address'] = current_year_df['Source_Address']/total_yearly_count*100\n",
    "        scaled_sources_by_year_country = pd.concat([scaled_sources_by_year_country, current_year_df])\n",
    "\n",
    "\n",
    "    scaled_sources_by_year_country = scaled_sources_by_year_country.loc[scaled_sources_by_year_country['Country'].isin(countries_of_interest)]\n",
    "\n",
    "    pd.pivot_table(scaled_sources_by_year_country, values = 'Source_Address', index = ['Country'], columns = ['Year']).plot.bar(figsize=(18,6), title='Percentage Unique Source Addresses by Country')\n",
    "\n",
    "    if save_figs == 1:\n",
    "        plt.savefig(f'{output_dir_path}Unique_Country_SourceAddresses_Percents.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    \n",
    "    # Find Top Countries by Number and Precentage of Sources\n",
    "    unique_sources_by_year_country = subset_table.groupby(['Year', 'Country'])['Source_Address'].nunique().reset_index()\n",
    "\n",
    "    top_num_countries = pd.DataFrame()\n",
    "    top_percent_countries = pd.DataFrame()\n",
    "\n",
    "    for year in years_of_interest:\n",
    "        current_year_df = unique_sources_by_year_country[unique_sources_by_year_country['Year']==year]\n",
    "        top_10_num_sources = current_year_df[['Year', 'Country', 'Source_Address']].sort_values('Source_Address', ascending=False).nlargest(10, 'Source_Address')\n",
    "        top_num_countries = pd.concat([top_num_countries, top_10_num_sources])\n",
    "        display(top_10_num_sources)\n",
    "    #     top_10_num_sources.head(10)\n",
    "\n",
    "        total_yearly_count = current_year_df['Source_Address'].sum()\n",
    "        current_year_df['Source_Address'] = current_year_df['Source_Address']/total_yearly_count*100\n",
    "        top_10_percent_sources = current_year_df[['Year', 'Country', 'Source_Address']].sort_values('Source_Address', ascending=False).nlargest(10, 'Source_Address')\n",
    "        top_percent_countries = pd.concat([top_percent_countries, top_10_percent_sources])\n",
    "        display(top_10_percent_sources)\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(f'{output_dir_path}Top10Countries.xlsx') as writer:\n",
    "        top_num_countries.to_excel(writer, sheet_name='Counts', index = False)\n",
    "        top_percent_countries.to_excel(writer, sheet_name='Percentages', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    print('Top countries By Unique Source Addresses')\n",
    "    display(top_num_countries)\n",
    "    print('Top countries By Unique Source Address Percentages')\n",
    "    display(top_percent_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    unique_cities_by_country = subset_table.groupby(['Country'])['City'].nunique().reset_index()\n",
    "\n",
    "    unique_cities_by_country = unique_cities_by_country.loc[unique_cities_by_country['Country'].isin(countries_of_interest)].set_index('Country')\n",
    "\n",
    "    unique_cities_by_country = unique_cities_by_country.rename(columns={'City': 'Number of Cities'})\n",
    "\n",
    "    unique_cities_by_country.plot.bar(figsize=(18,6), title='Number of Cities in Each Country')\n",
    "\n",
    "    if save_figs == 1:\n",
    "        plt.savefig(f'{output_dir_path}Unique_Country_Cities.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    \n",
    "    # Find Top Cities by Number and Precentage of Sources\n",
    "    unique_sources_by_year_city = subset_table.groupby(['Year', 'Country', 'City'])['Source_Address'].nunique().reset_index()\n",
    "    num_top_cities = 15\n",
    "\n",
    "    top_num_cities = pd.DataFrame()\n",
    "    top_percent_cities = pd.DataFrame()\n",
    "\n",
    "    for year in years_of_interest:\n",
    "        current_year_df = unique_sources_by_year_city[unique_sources_by_year_city['Year']==year]\n",
    "        top_10_num_sources = current_year_df[['Year', 'Country', 'City', 'Source_Address']].sort_values('Source_Address', ascending=False).nlargest(num_top_cities, 'Source_Address')\n",
    "        top_num_cities = pd.concat([top_num_cities, top_10_num_sources])\n",
    "        print(f'{year} Top countries By Unique Source Addresses')\n",
    "        display(top_10_num_sources)\n",
    "\n",
    "        total_yearly_count = current_year_df['Source_Address'].sum()\n",
    "        current_year_df['Source_Address'] = current_year_df['Source_Address']/total_yearly_count*100\n",
    "        top_10_percent_sources = current_year_df[['Year', 'Country', 'City', 'Source_Address']].sort_values('Source_Address', ascending=False).nlargest(num_top_cities, 'Source_Address')\n",
    "        top_percent_cities = pd.concat([top_percent_cities, top_10_percent_sources])\n",
    "        print(f'{year} Top countries By Unique Source Address Percentages')\n",
    "        display(top_10_percent_sources)\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(f'{output_dir_path}Top{num_top_cities}Cities.xlsx') as writer:\n",
    "        top_num_cities.to_excel(writer, sheet_name='Counts', index = False)\n",
    "        top_percent_cities.to_excel(writer, sheet_name='Percentages', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the Devices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_merged_table = []\n",
    "\n",
    "if analyze_device_types == 1:\n",
    "    print('Reading and combining the device types data')\n",
    "\n",
    "    devices_df = pd.read_csv(devices_db_file)\n",
    "    devices_df.drop(['Original_Device_Info', 'Timestamp', 'Device_Info'], axis=1, inplace=True)\n",
    "    devices_df = devices_df.rename(columns={'IP': 'Source_Address'})\n",
    "\n",
    "    if debug == 1:\n",
    "        display(devices_df)\n",
    "\n",
    "    if analyze_geolocation == 1:\n",
    "        devices_merged_table = pd.merge(subset_table, devices_df, on=\"Source_Address\", how = 'inner')\n",
    "    else:\n",
    "        devices_merged_table = pd.merge(orig_data, devices_df, on=\"Source_Address\", how = 'inner')\n",
    "\n",
    "    if debug == 1:\n",
    "        display(devices_merged_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_device_types == 1:\n",
    "\n",
    "    def gen_colors(df):\n",
    "        col_d = {'Server': 'maroon', 'Router': 'blue', 'Recorder': 'black', 'Camera': 'pink', 'NVR': 'green', \n",
    "                 'HVR': 'yellow', 'DVR': 'orange'}\n",
    "        return [col_d[col] for col in df.columns]\n",
    "    \n",
    "    devices_by_year = devices_merged_table.groupby(['Year', 'Device_Type'])['Source_Address'].nunique().reset_index()\n",
    "    \n",
    "    pivot_data = pd.pivot_table(devices_by_year, values = 'Source_Address', index = ['Year'], columns = ['Device_Type'])\n",
    "    pivot_data.plot(kind = \"bar\", stacked = True, color=gen_colors(pivot_data), figsize=(18,6), \n",
    "                    grid = True, title=f'{year} Device Types by Years')\n",
    "    \n",
    "    if save_figs == 1:\n",
    "        plt.savefig(f'{output_dir_path}Yearly_devices.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate devices plots for all countries\n",
    "if (analyze_device_types == 1) and (analyze_geolocation == 1):\n",
    "\n",
    "    devices_by_country = devices_merged_table.groupby(['Year', 'Country', 'Device_Type'])['Source_Address'].nunique().reset_index()\n",
    "    max_address_count = devices_by_country['Source_Address'].max()\n",
    "    max_address_count += 5\n",
    "    \n",
    "    # Unique source counts by year and country\n",
    "    for year in years_of_interest:\n",
    "        current_table = devices_by_country[devices_by_country['Year']==year]\n",
    "\n",
    "        pivot_data = pd.pivot_table(current_table, values = 'Source_Address', index = ['Country'], columns = ['Device_Type'])\n",
    "        pivot_data.plot(kind = \"bar\", stacked = True, color=gen_colors(pivot_data), figsize=(18,6), \n",
    "                        ylim=(0, max_address_count), grid = True, title=f'{year} Device Types for All Countries')\n",
    "#         plt.set_ylim(0, max_address_count)\n",
    "        \n",
    "        if save_figs == 1:\n",
    "            plt.savefig(f'{output_dir_path}Country_devices_{year}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate devices plots for only the countries of intrest\n",
    "\n",
    "if (analyze_device_types == 1) and (analyze_geolocation == 1):\n",
    "    # Unique source counts by year and country\n",
    "    \n",
    "    devices_by_country = devices_by_country.loc[devices_by_country['Country'].isin(countries_of_interest)]\n",
    "    max_address_count = devices_by_country['Source_Address'].max()\n",
    "    max_address_count += 5\n",
    "        \n",
    "    for year in years_of_interest:\n",
    "        current_table = devices_by_country[devices_by_country['Year']==year]\n",
    "\n",
    "        pivot_data = pd.pivot_table(current_table, values = 'Source_Address', index = ['Country'], columns = ['Device_Type'])\n",
    "        pivot_data.plot(kind = \"bar\", stacked = True, color=gen_colors(pivot_data), figsize=(18,6),\n",
    "                        ylim=(0, max_address_count), grid = True, title=f'{year} Device Types for Countries of Interest')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_figs == 1:\n",
    "            plt.savefig(f'{output_dir_path}CountryOfInterest_devices_{year}.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
