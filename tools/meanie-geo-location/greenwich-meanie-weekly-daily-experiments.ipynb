{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2020-2024 - Raytheon BBN Technologies Corp.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "\n",
    "You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0.\n",
    "\n",
    "Unless required by applicable law or agreed to in writing,\n",
    "software distributed under the License is distributed on an\n",
    "\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n",
    "either express or implied. See the License for the specific\n",
    "language governing permissions and limitations under the License.\n",
    "\n",
    "Distribution Statement \"A\" (Approved for Public Release,\n",
    "Distribution Unlimited).\n",
    "\n",
    "This material is based upon work supported by the Defense\n",
    "Advanced Research Projects Agency (DARPA) under Contract No.\n",
    "HR001119C0102.  The opinions, findings, and conclusions stated\n",
    "herein are those of the authors and do not necessarily reflect\n",
    "those of DARPA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greenwich Meanie Daily and Weekly Analysis on a Specified /24 Destination Subnet\n",
    "\n",
    "Runs analysis on the Meanine data to see Weekly and Daily patterns on a given /24 subnet.\n",
    "\n",
    "<b>Input data provided should only include one week per year<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T18:50:01.257049544Z",
     "start_time": "2023-08-14T18:50:01.180694966Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipaddress\n",
    "import os\n",
    "import math\n",
    "from textwrap import wrap\n",
    "\n",
    "# Ignoring warnings, otherwise there are warnings about having too many graphs\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set value to 1 to see intermediate outputs for debugging. 0 otherwise (recommended)\n",
    "debug = 0\n",
    "\n",
    "# /24 of the destination addresses to be kept\n",
    "destination_address_of_interest = '10.1.202'\n",
    "\n",
    "# Set value to 1 to do geolocation analysis using results generated with 'ipinfo.io'. 0 otherwise (recommended)\n",
    "# If this is 1, add the path to the geolocations database in the next variable \n",
    "analyze_geolocation = 0\n",
    "\n",
    "# full path and filename that contains prior Geolocation database\n",
    "geo_location_db_file = 'db/full_geolocations_db.csv'\n",
    "\n",
    "# Parent directory where meanie text data is stored\n",
    "input_dir_path = r'/home/nice-user/Weekly'\n",
    "\n",
    "# Directory where meanie results are printed\n",
    "output_dir_path = r'/home/nice-user/Weekly'\n",
    "\n",
    "# Set value to 1 to save figures to output_dir_path. 0 otherwise\n",
    "save_figs = 1\n",
    "\n",
    "# Year in the data \n",
    "years_of_interest = [2020, 2021, 2022, 2023]\n",
    "\n",
    "# Countries we want to extract Information for\n",
    "countries_of_interest = [\"United Kingdom\", \"Philippines\", \"Brazil\", \"India\", \"Greece\", \"United States\", \"Canada\", \n",
    "                         \"Serbia\", \"Poland\", \"India\", \"Croatia\", \"Indonesia\", \"South Africa\", \"China\", \"Russia\", \n",
    "                         \"Singapore\", \"Argentina\", \"Australia\", \"Czech Republic\", \"Bangladesh\", \"France\", \"Malaysia\", \"Turkey\"]\n",
    "\n",
    "# Timezone offset for countries_of_interest relative to the US Eastern Timezone\n",
    "countries_timezones = {\"United Kingdom\": 5, \"Philippines\": 12, \"Brazil\": 1, \"India\": 9.5, \"Greece\": 7, \n",
    "                       \"United States\": -1, \"Canada\": -1, \"Serbia\": 6, \"Poland\": 6, \"Croatia\": 6, \n",
    "                       \"Indonesia\": 12, \"South Africa\": 6, \"China\": 12, \"Russia\":7, \"Singapore\": 13, \n",
    "                       \"Argentina\": 2, \"Australia\": 14, \"Czech Republic\":6, \"Bangladesh\":10, \"France\": 6, \n",
    "                       \"Malaysia\": 12, \"Turkey\": 7}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Variables and Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Columns in the meanie text files that have data as hex\n",
    "hex_columns = ['Source_Address', 'Destination_Address', 'Source_Port', 'Destination_Port', 'UDP_Checksum', 'TTL', 'IPID']\n",
    "\n",
    "# Columns from meanie text that will need to be formatted as IP addresses\n",
    "ip_columns = ['Source_Address', 'Destination_Address']\n",
    "\n",
    "# Columns for computing before and after filtering statistics\n",
    "stat_columns = ['Source_Address', 'Destination_Address', 'TTL', 'Destination_Port', 'Payload_Length', 'Destination_Slash24']\n",
    "\n",
    "orig_data = pd.DataFrame()\n",
    "data_stats = {}\n",
    "\n",
    "for year in years_of_interest:\n",
    "    data_stats[year] = {}\n",
    "\n",
    "if(input_dir_path[len(input_dir_path)-1] != '/'):\n",
    "    input_dir_path = input_dir_path + '/'\n",
    "    \n",
    "if(output_dir_path[len(output_dir_path)-1] != '/'):\n",
    "    output_dir_path = output_dir_path + '/'\n",
    "\n",
    "all_dest_addresses = [f'{destination_address_of_interest}.{x}' for x in range(0,256)]\n",
    "\n",
    "if debug == 1:\n",
    "    print(input_dir_path)\n",
    "    print(data_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes Stats on the data before and after filtering\n",
    "\n",
    "def compute_data_stats(orig_data, years_of_interest, stat_columns, prefix):\n",
    "    for year in years_of_interest:\n",
    "        current_year_data = orig_data[orig_data['Year'] == year]\n",
    "        current_year_unique_months = current_year_data['Month'].nunique()\n",
    "        if debug == 1:\n",
    "            print(year, ' unique months: ', current_year_unique_months)\n",
    "        data_stats[year][f'{prefix}_Packet_Counts'] = len(current_year_data)\n",
    "        data_stats[year][f'{prefix}_Packet_norm'] = data_stats[year][f'{prefix}_Packet_Counts'] / current_year_unique_months\n",
    "        for column in stat_columns:\n",
    "            data_stats[year][f'{prefix}_Unique_{column}_Counts'] = current_year_data[column].nunique()\n",
    "            data_stats[year][f'{prefix}_Unique_{column}s'] = current_year_data[column].unique()\n",
    "            data_stats[year][f'{prefix}_Unique_{column}_norm'] = data_stats[year][f'{prefix}_Unique_{column}_Counts'] / current_year_unique_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read / Clean / Format / Merge Data and Compute Filtering Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the Meanie Text Data\n",
    "# values in Input data\n",
    "\n",
    "# Saddr - Hex\n",
    "# Daddr - Hex\n",
    "# Sport - Hex\n",
    "# Dport - Hex\n",
    "# Proto – always UDP (17)\n",
    "# Timestamp\n",
    "# UdpCksum - Hex\n",
    "# PayloadLen – the length of the payload, in bytes (the UDP length field, minus 8 for the UDP header itself)\n",
    "# Payload – Hex\n",
    "# TTL - a hex\n",
    "# IPID - a hex\n",
    "\n",
    "# for folder in data_folders:\n",
    "print('Reading Meanie Data')\n",
    "\n",
    "# list to store files\n",
    "file_list = []\n",
    "\n",
    "# Iterate directory and find all compatible files\n",
    "for file in os.listdir(input_dir_path):\n",
    "    # check only text files and add them to the list\n",
    "    if file.endswith('.txt'):\n",
    "        file_list.append(file)\n",
    "\n",
    "# if there is 1 or more file type of interest\n",
    "if len(file_list)>=1:\n",
    "\n",
    "    for file in range(0, len(file_list)):\n",
    "        full_file_path = input_dir_path+file_list[file]\n",
    "\n",
    "        # Read data from the file only if the file is not empty, i.e. file size > 0\n",
    "        if os.path.getsize(full_file_path) > 0:\n",
    "            current_csv = pd.read_csv(full_file_path, header=None)\n",
    "            current_csv['FileName'] = file_list[file]\n",
    "            orig_data = pd.concat([orig_data, current_csv], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    print(\"No compatible Files in the directory\")\n",
    "\n",
    "if debug == 1:\n",
    "    display(orig_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if the data is not empty\n",
    "if len(orig_data)>=1:\n",
    "\n",
    "    # Format all the read data\n",
    "    orig_data = orig_data.rename(columns={0: 'Source_Address', 1:'Destination_Address', 2:'Source_Port', \n",
    "                                          3:'Destination_Port', 4:'Protocol', 5:'Packet_Timestamp', \n",
    "                                          6:'UDP_Checksum', 7:'Payload_Length', 8:'Payload', 9:'TTL', 10:'IPID'})\n",
    "    \n",
    "    for hc in hex_columns:\n",
    "        orig_data[hc] = orig_data[hc].apply(lambda x: int(x, 16))\n",
    "        \n",
    "    for ic in ip_columns:\n",
    "        orig_data[ic] = orig_data[ic].apply(lambda x: str(ipaddress.ip_address(x)))\n",
    "        \n",
    "    orig_data[['Prefix', 'Year', 'Month', 'Day', 'Hour', 'Post']] = orig_data['FileName'].str.split('-', expand=True)\n",
    "    orig_data = orig_data.drop(['Prefix', 'Post'], axis=1)\n",
    "    orig_data['Data_Date'] = orig_data['Year'] + '-' + orig_data['Month'] + '-' + orig_data['Day']\n",
    "    orig_data['Data_Date'] = pd.to_datetime(orig_data['Data_Date'], format='%Y-%m-%d').dt.date\n",
    "    orig_data[['Year', 'Month', 'Day', 'Hour']] = orig_data[['Year', 'Month', 'Day', 'Hour']].astype(int)\n",
    "    orig_data['Destination_Slash24'] = orig_data['Destination_Address'].apply(lambda row: '.'.join(row.split('.')[:3]))\n",
    "    \n",
    "    orig_data['Packet_Timestamp'] = [datetime.fromtimestamp(x) for x in list(orig_data['Packet_Timestamp'])]\n",
    "    \n",
    "    \n",
    "    if debug == 1:\n",
    "        display(orig_data)\n",
    "    \n",
    "    # Extract Stats about the full data\n",
    "    compute_data_stats(orig_data, years_of_interest, stat_columns, 'All')\n",
    "    \n",
    "    if debug == 1:\n",
    "        print('Before Filtering')\n",
    "        for year in years_of_interest:\n",
    "            print(f'{year} Num Unique Destination Addresses: ', data_stats[year]['All_Unique_Destination_Address_Counts'])\n",
    "    \n",
    "    # Keep a copy of the full data without filtering\n",
    "    unfiltered_data = orig_data\n",
    "    \n",
    "    \n",
    "    # filter the data by subnet of interest\n",
    "    orig_data = orig_data[orig_data[\"Destination_Address\"].isin(all_dest_addresses)]\n",
    "    # Extract Stats about the filtered data\n",
    "    compute_data_stats(orig_data, years_of_interest, stat_columns, 'Filtered')\n",
    "    \n",
    "    if debug == 1:\n",
    "        print('\\nAfter Filtering')\n",
    "        for year in years_of_interest:\n",
    "            print(f'{year} Num Unique Destination Addresses: ', data_stats[year]['Filtered_Unique_Destination_Address_Counts'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Geolocations Database\n",
    "if analyze_geolocation == 1:\n",
    "    print(\"Merging with GeoLocations data\")\n",
    "\n",
    "    database_geo_locations = pd.read_csv(geo_location_db_file)\n",
    "    database_geo_locations = database_geo_locations.rename(columns={'IP': 'Source_Address'})\n",
    "\n",
    "    if debug == 1:\n",
    "        display(database_geo_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    if debug == 1:\n",
    "        database_geo_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two tables \n",
    "if analyze_geolocation == 1:\n",
    "    merged_table = pd.merge(orig_data, database_geo_locations, on=\"Source_Address\", how = 'left')\n",
    "\n",
    "    if debug == 1:\n",
    "        display(merged_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    if debug == 1:\n",
    "        merged_table.info()\n",
    "        merged_table['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary columns\n",
    "if analyze_geolocation == 1:\n",
    "    subset_table = merged_table.drop(['Timestamp', 'Payload'], axis=1)\n",
    "\n",
    "    if debug == 1:\n",
    "        print('subset_table.size', subset_table.size)\n",
    "        subset_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows so each source exists only once for each year and location \n",
    "if analyze_geolocation == 1:\n",
    "    subset_table = subset_table.drop_duplicates()\n",
    "\n",
    "    if debug == 1:\n",
    "        print('subset_table.size', subset_table.size)\n",
    "        display(subset_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    if debug == 1:\n",
    "        subset_table.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Geolocation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    continent_coordinates = {'World': {'xlim':[-180, 180], 'ylim':[-90, 90]},\n",
    "                             'Europe': {'xlim':[-20, 50], 'ylim':[25, 70]},\n",
    "                             'Asia': {'xlim':[60, 145], 'ylim':[-10, 50]},\n",
    "                             'North America': {'xlim':[-130, -60], 'ylim':[15, 60]},\n",
    "                             'South America': {'xlim':[-85, -30], 'ylim':[-55, 15]},\n",
    "                             'Africa': {'xlim':[-35, 60], 'ylim':[-40,40]},\n",
    "                             'Australia': {'xlim':[110, 187], 'ylim':[-50, -5]}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    unique_source_counts_by_year_location = subset_table.groupby(['Year', 'Country', 'Region', 'City', 'Latitude', 'Longitude'])['Source_Address'].nunique().reset_index()\n",
    "    unique_source_counts_by_year_location.to_csv(f'{output_dir_path}CityCountsByWeek.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_geolocation == 1:\n",
    "    unique_years = unique_source_counts_by_year_location['Year'].unique()\n",
    "    max_count = unique_source_counts_by_year_location['Source_Address'].max()\n",
    "\n",
    "    continents = list(continent_coordinates.keys())\n",
    "    for current_year in unique_years:\n",
    "        current_year_df = unique_source_counts_by_year_location.loc[unique_source_counts_by_year_location['Year']==current_year]\n",
    "        min_date = subset_table.loc[subset_table['Year']==current_year, 'Data_Date'].min()\n",
    "        max_date = subset_table.loc[subset_table['Year']==current_year, 'Data_Date'].max()\n",
    "\n",
    "        for continent in continents:\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "            # Plotting world map\n",
    "            worldmap = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "            worldmap.plot(color=\"lightgrey\", ax=ax)\n",
    "            ax.set_axis_off()\n",
    "\n",
    "            longitudes = current_year_df['Longitude']\n",
    "            latitudes = current_year_df['Latitude']\n",
    "            counts = current_year_df['Source_Address']\n",
    "            # Plotting longitudes and latitudes\n",
    "            plt.scatter(longitudes, latitudes, s=counts, c=counts, alpha=0.6, vmin=0, vmax=max_count, cmap='autumn')\n",
    "            plt.colorbar(label=\"Count of Unique Source Addresses by City\")\n",
    "            plt.xlim(continent_coordinates[continent]['xlim'])\n",
    "            plt.ylim(continent_coordinates[continent]['ylim'])\n",
    "            plt.title(f'{continent}\\n{min_date} to {max_date}')\n",
    "            if save_figs == 1:\n",
    "                plt.savefig(f'{output_dir_path}{continent}_map_{current_year}.png')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASN Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which ASNs have 5% of the data or more for that country and then generate pie chart. \n",
    "# All ASNs with less than 5% of the data are grouped into the 'Other' category for ASNs\n",
    "if analyze_geolocation == 1:\n",
    "    \n",
    "    for country, timezone in countries_timezones.items():\n",
    "\n",
    "        current_country_df = subset_table[subset_table['Country'] == country]\n",
    "        current_country_df.loc[:,'Hour'] = (current_country_df['Packet_Timestamp'] + pd.Timedelta(hours=timezone)).dt.hour\n",
    "\n",
    "        country_year_packet_counts = current_country_df.groupby(['Year', 'AS_Name', 'Hour'])['Packet_Timestamp'].count().reset_index()\n",
    "        country_year_packet_counts = country_year_packet_counts.rename(columns={'Packet_Timestamp': 'Packet_Count', \n",
    "                                                                                'AS_Name':'AS Name'})\n",
    "\n",
    "        for year in years_of_interest:\n",
    "\n",
    "            current_year_df = country_year_packet_counts[country_year_packet_counts['Year'] == year]\n",
    "\n",
    "            for asn in current_year_df['AS Name'].unique():\n",
    "                current_asn_df_sum = sum(current_year_df.loc[current_year_df['AS Name'] == asn, 'Packet_Count'])\n",
    "                if current_asn_df_sum < 0.05 * len(current_year_df):    \n",
    "                    current_year_df.loc[current_year_df['AS Name'] == asn, 'AS Name'] = 'Other'\n",
    "\n",
    "\n",
    "            asn_packet_counts = current_year_df.groupby(['AS Name'])['Packet_Count'].sum().reset_index()\n",
    "            asn_packet_counts = asn_packet_counts.set_index('AS Name')\n",
    "\n",
    "            fig, ax = plt.subplots() \n",
    "            title = plt.title(f'{year} ASN Breakdown in {country}')\n",
    "            title.set_ha(\"left\")\n",
    "            plt.gca().axis(\"equal\")\n",
    "            pie = plt.pie(asn_packet_counts['Packet_Count'], startangle=0)\n",
    "            labels = [ '\\n'.join(wrap(l, 40)) for l in asn_packet_counts.index]\n",
    "            plt.legend(pie[0],labels, bbox_to_anchor=(1,0.5), loc=\"center right\", fontsize=10, bbox_transform=plt.gcf().transFigure)\n",
    "            plt.subplots_adjust(left=0.0, bottom=0.1, right=0.45)\n",
    "            if save_figs == 1:\n",
    "                plt.savefig(f'{output_dir_path}{country}_ASN_Breakdown_{year}.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly Plots\n",
    "Counts of all the packets for the week by each hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Data\n",
    "\n",
    "year_hour_packet_counts = orig_data.groupby(['Year', 'Hour'])['Packet_Timestamp'].count().reset_index()\n",
    "year_hour_packet_counts = year_hour_packet_counts.rename(columns={'Packet_Timestamp': 'Packet_Count'})\n",
    "\n",
    "rows = 2\n",
    "cols = math.ceil(len(years_of_interest) / rows)\n",
    "\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(rows, cols, sharex = True, sharey=True, figsize=(12,6))\n",
    "fig.suptitle(f'Hourly Breakdowns Worldwide')\n",
    "\n",
    "for year in range(0, len(years_of_interest)):\n",
    "    current_year = years_of_interest[year]\n",
    "    min_date = orig_data.loc[orig_data['Year']==current_year, 'Data_Date'].min()\n",
    "    max_date = orig_data.loc[orig_data['Year']==current_year, 'Data_Date'].max()\n",
    "\n",
    "    hourly_packet_counts = year_hour_packet_counts[year_hour_packet_counts['Year'] == current_year]\n",
    "\n",
    "    ax_row = math.floor(year/cols)\n",
    "    ax_col = year%cols\n",
    "    axs[ax_row, ax_col].bar(hourly_packet_counts['Hour'], hourly_packet_counts['Packet_Count'])\n",
    "    axs[ax_row, ax_col].set_title(f'Hourly {min_date} to {max_date}')\n",
    "    axs[ax_row, ax_col].grid()\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=f\"Hour\", ylabel=\"Packet count\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Global_Hourly.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Country \n",
    "\n",
    "if analyze_geolocation == 1:\n",
    "    \n",
    "    for country, timezone in countries_timezones.items():\n",
    "\n",
    "        current_country_df = subset_table[subset_table['Country'] == country]\n",
    "        current_country_df.loc[:, 'Hour'] = (current_country_df['Packet_Timestamp'] + pd.Timedelta(hours=timezone)).dt.hour\n",
    "\n",
    "        country_year_packet_counts = current_country_df.groupby(['Year', 'AS_Name', 'Hour'])['Packet_Timestamp'].count().reset_index()\n",
    "        country_year_packet_counts = country_year_packet_counts.rename(columns={'Packet_Timestamp': 'Packet_Count', \n",
    "                                                                                'AS_Name':'AS Name'})\n",
    "\n",
    "        rows = 2\n",
    "        cols = math.ceil(len(years_of_interest) / rows)\n",
    "\n",
    "        plt.figure()\n",
    "        fig, axs = plt.subplots(rows, cols, sharex = True, sharey=True, figsize=(12,6))\n",
    "        fig.suptitle(f'{country} Hourly Breakdowns')\n",
    "\n",
    "        for year in range(0, len(years_of_interest)):\n",
    "            current_year = years_of_interest[year]\n",
    "            min_date = subset_table.loc[subset_table['Year']==current_year, 'Data_Date'].min()\n",
    "            max_date = subset_table.loc[subset_table['Year']==current_year, 'Data_Date'].max()\n",
    "\n",
    "            current_year_df = country_year_packet_counts[country_year_packet_counts['Year'] == current_year]\n",
    "            hourly_packet_counts = current_year_df.groupby(['Hour'])['Packet_Count'].sum().reset_index()\n",
    "\n",
    "            ax_row = math.floor(year/cols)\n",
    "            ax_col = year%cols\n",
    "            axs[ax_row, ax_col].bar(hourly_packet_counts['Hour'], hourly_packet_counts['Packet_Count'])\n",
    "            axs[ax_row, ax_col].set_title(f'Hourly {min_date} to {max_date}')\n",
    "            axs[ax_row, ax_col].grid()\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set(xlabel=f\"Hour (EST{'+' if timezone >= 0 else ''}{timezone})\", ylabel=\"Packet count\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.label_outer()\n",
    "\n",
    "        if save_figs == 1:\n",
    "            plt.savefig(f'{output_dir_path}{country}_Hourly.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Plots\n",
    "Counts of all the packets daily grouped by 3 hour intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Data\n",
    "\n",
    "\n",
    "current_df = orig_data\n",
    "\n",
    "current_timestamps = list(current_df['Packet_Timestamp']) \n",
    "new_timestamps = [int((x.weekday() * 24 + x.hour) / 3) for x in current_timestamps]\n",
    "current_df.loc[:, 'Hour'] = new_timestamps\n",
    "\n",
    "year_hour_packet_counts = current_df.groupby(['Year', 'Hour'])['Packet_Timestamp'].count().reset_index()\n",
    "year_hour_packet_counts = year_hour_packet_counts.rename(columns={'Packet_Timestamp': 'Packet_Count'})\n",
    "\n",
    "rows = 2\n",
    "cols = math.ceil(len(years_of_interest) / rows)\n",
    "\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(rows, cols, sharex = True, sharey=True, figsize=(12,6))\n",
    "fig.suptitle(f'Weekly Breakdowns Worldwide')\n",
    "\n",
    "for year in range(0, len(years_of_interest)):\n",
    "    current_year = years_of_interest[year]\n",
    "    min_date = orig_data.loc[orig_data['Year']==current_year, 'Data_Date'].min()\n",
    "    max_date = orig_data.loc[orig_data['Year']==current_year, 'Data_Date'].max()\n",
    "\n",
    "    hourly_packet_counts = year_hour_packet_counts[year_hour_packet_counts['Year'] == current_year]\n",
    "#         hourly_packet_counts = current_year_df.groupby(['Hour'])['Packet_Count'].sum().reset_index()\n",
    "\n",
    "    ax_row = math.floor(year/cols)\n",
    "    ax_col = year%cols\n",
    "    axs[ax_row, ax_col].bar(hourly_packet_counts['Hour'], hourly_packet_counts['Packet_Count'])\n",
    "    axs[ax_row, ax_col].set_title(f'Weekly {min_date} to {max_date}')\n",
    "    axs[ax_row, ax_col].grid()\n",
    "    axs[ax_row, ax_col].tick_params(labelrotation=30)        \n",
    "\n",
    "plt.setp(axs, xticks=list(range(4, 7*8 +1, 8)), xticklabels=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel=f\"Weekday\", ylabel=\"Packet count\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "if save_figs == 1:\n",
    "    plt.savefig(f'{output_dir_path}Global_Weekly.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Country\n",
    "\n",
    "if analyze_geolocation == 1:\n",
    "\n",
    "    for country, timezone in countries_timezones.items():\n",
    "\n",
    "        current_country_df = subset_table[subset_table['Country'] == country]\n",
    "\n",
    "        current_timestamps = list(current_country_df['Packet_Timestamp']) \n",
    "        new_timestamps = [int((x.weekday() * 24 + x.hour) / 3) for x in current_timestamps]\n",
    "        current_country_df.loc[:, 'Hour'] = new_timestamps\n",
    "\n",
    "        country_year_packet_counts = current_country_df.groupby(['Year', 'AS_Name', 'Hour'])['Packet_Timestamp'].count().reset_index()\n",
    "        country_year_packet_counts = country_year_packet_counts.rename(columns={'Packet_Timestamp': 'Packet_Count', \n",
    "                                                                                'AS_Name':'AS Name'})\n",
    "\n",
    "        rows = 2\n",
    "        cols = math.ceil(len(years_of_interest) / rows)\n",
    "\n",
    "        plt.figure()\n",
    "        fig, axs = plt.subplots(rows, cols, sharex = True, sharey=True, figsize=(12,6))\n",
    "        fig.suptitle(f'{country} Weekly Breakdowns')\n",
    "\n",
    "        for year in range(0, len(years_of_interest)):\n",
    "            current_year = years_of_interest[year]\n",
    "            min_date = subset_table.loc[subset_table['Year']==current_year, 'Data_Date'].min()\n",
    "            max_date = subset_table.loc[subset_table['Year']==current_year, 'Data_Date'].max()\n",
    "\n",
    "            current_year_df = country_year_packet_counts[country_year_packet_counts['Year'] == current_year]\n",
    "            hourly_packet_counts = current_year_df.groupby(['Hour'])['Packet_Count'].sum().reset_index()\n",
    "\n",
    "            ax_row = math.floor(year/cols)\n",
    "            ax_col = year%cols\n",
    "            axs[ax_row, ax_col].bar(hourly_packet_counts['Hour'], hourly_packet_counts['Packet_Count'])\n",
    "            axs[ax_row, ax_col].set_title(f'Weekly {min_date} to {max_date}')\n",
    "            axs[ax_row, ax_col].grid()\n",
    "            axs[ax_row, ax_col].tick_params(labelrotation=30)        \n",
    "\n",
    "        plt.setp(axs, xticks=list(range(4, 7*8 +1, 8)), xticklabels=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.set(xlabel=f\"Weekday\", ylabel=\"Packet count\")\n",
    "\n",
    "        for ax in axs.flat:\n",
    "            ax.label_outer()\n",
    "\n",
    "        if save_figs == 1:\n",
    "            plt.savefig(f'{output_dir_path}{country}_Weekly.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
